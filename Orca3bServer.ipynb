{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6VbqwGgkLTXyJC6e/iAcq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelmarais/colab-llm-inference-server/blob/main/Orca3bServer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In this notebook you will learn how to:\n",
        "\n",
        "\n",
        "1. Serve a streaming LLM (Orca-mini 3b) locally in a notebook using Ollama.\n",
        "2. Expose a port using `ngrok` so that you can access it from anywhere!\n",
        "\n",
        "**Note**: you will need a GPU for this to work. To do this go to `Runtime` > `Change runtime type` and select T4 GPU."
      ],
      "metadata": {
        "id": "WeM-b2SUslRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'orca-mini:3b-q4_1'\n",
        "OLLAMA_PORT = '11434'"
      ],
      "metadata": {
        "id": "Xoi0Jy2g3PpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Ollama\n",
        "\n",
        "[Ollama](https://github.com/jmorganca/ollama) allows you to run LLMs locally!\n",
        "\n",
        "It supports some [popular models](https://ollama.ai/library) but the larger models don't work on the colab free tier."
      ],
      "metadata": {
        "id": "pk1Q7oWFqkPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install lshw\n",
        "!sudo apt install pciutils\n",
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVMU73uKeM2a",
        "outputId": "a6e20ab9-6458-4300-8b11-19fe9cf2c586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVLd1vy0fnXY",
        "outputId": "2c71b0b9-7332-4826-b036-9605b42d39b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama run orca-mini:3b-q4_1&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXL0mWoKejNT",
        "outputId": "3f642422-d52a-4279-a802-8dbc6eeb9e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def generate_text(prompt, words_per_line=12):\n",
        "    url = f\"http://localhost:{OLLAMA_PORT}/api/generate\"\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"prompt\": prompt\n",
        "    }\n",
        "\n",
        "    word_count = 0\n",
        "\n",
        "    with requests.post(url, json=payload, stream=True) as response:\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                response_text = json.loads(line.decode('utf-8'))['response']\n",
        "                print(response_text, end=' ')\n",
        "                word_count += 1\n",
        "\n",
        "                if word_count % words_per_line == 0:\n",
        "                    print()\n"
      ],
      "metadata": {
        "id": "eaec6NwBfSa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "generate_text(\"Should you use Colab notebooks as AI infrastructure\")"
      ],
      "metadata": {
        "id": "PmEmFTNcg_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the server availible outside of the notebook\n",
        "\n",
        "To do this we'll use **ngrok** which: \"enables developers to expose a local development server to the Internet with minimal effort.\"\n"
      ],
      "metadata": {
        "id": "9YbCEV4FqxuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install ngrok\n",
        "!wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -qq -n ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "iSx6NFD1qwT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw(f'./ngrok http {OLLAMA_PORT} &')"
      ],
      "metadata": {
        "id": "Ds4i9QPBq2bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Print the public URL for your inference server\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSwfMjofq82l",
        "outputId": "7fdc2989-980f-4e24-e79a-b7fbfc25ec84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://4d90-34-125-145-23.ngrok.io\n"
          ]
        }
      ]
    }
  ]
}